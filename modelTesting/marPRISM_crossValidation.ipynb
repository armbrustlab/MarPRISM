{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedShuffleSplit\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "from xgboost import XGBClassifier\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#load training data and labels\n",
    "train = pd.read_csv('../trainingDataMarPRISM.csv')\n",
    "\n",
    "#get just the TPM values not the MMETSP entry IDs and trophic mode labels\n",
    "trainData = train.iloc[:, 2:]\n",
    "\n",
    "#load feature Pfams for model\n",
    "features = pd.read_csv('../MarPRISM_featurePfams.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract pfam column from features dataframe\n",
    "features = features['pfam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to encode trophic labels as numbers (0,1,2)\n",
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get just data for the feature Pfams from training data\n",
    "trainData = trainData[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign feature matrix and target vector\n",
    "X, y = trainData, le.fit_transform(train['Trophic mode'])\n",
    "#X: Feature matrix (independent variables) from the DataFrame trainData\n",
    "#y: Target vector (dependent variable), where the 'Trophic mode' column is label-encoded using `LabelEncoder`.\n",
    "\n",
    "#initialize a MinMaxScaler instance\n",
    "scaler = MinMaxScaler()\n",
    "#minMaxScaler scales features to a specified range, typically [0, 1], which can improve the performance of machine learning models\n",
    "\n",
    "#scale feature matrix\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the train_test_split function from scikit-learn library\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#split the dataset into training and testing subsets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,           # Features (input variables)\n",
    "    y,           # Target variable (output/labels)\n",
    "    test_size=0.4, # Proportion of the dataset to include in the test split (40%)\n",
    "    random_state=0 # Random seed for reproducibility of the split\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(7)\n",
    "\n",
    "#{'gamma': 0.0, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 1000, 'reg_lambda': 1.0} \n",
    "clf = XGBClassifier(gamma=0.0, learning_rate=0.1, max_depth=3, n_estimators=1000, reg_lambda=1.0)\n",
    "\n",
    "#define a custom scoring function for F1 score\n",
    "f1_scorer = make_scorer(f1_score, average='weighted')\n",
    "\n",
    "#create a StratifiedShuffleSplit cross-validator\n",
    "kf = StratifiedShuffleSplit(n_splits=6, random_state=7)\n",
    "\n",
    "#perform cross-validation and store the F1 scores\n",
    "f1_scores = cross_val_score(clf, X, y, cv=kf, scoring=f1_scorer)\n",
    "\n",
    "#save F1 scores, mean, and standard error to csv file\n",
    "data = {'F1_Scores': f1_scores}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "#save the DataFrame to a CSV file\n",
    "df.to_csv('marPRISM_f1_scores.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(7)\n",
    "\n",
    "#{'gamma': 0.0, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 1000, 'reg_lambda': 1.0} #\n",
    "clf = XGBClassifier(gamma=0.0, learning_rate=0.1, max_depth=3, n_estimators=1000, reg_lambda=1.0)\n",
    "\n",
    "#define a custom scoring function for F1 score\n",
    "def f1_scorer(y_true, y_pred):\n",
    "    return f1_score(y_true, y_pred, average=None)\n",
    "\n",
    "#create a StratifiedShuffleSplit cross-validator\n",
    "kf = StratifiedShuffleSplit(n_splits=6, random_state=7)\n",
    "\n",
    "#initialize an empty list to store F1 scores for each class\n",
    "all_f1_scores = []\n",
    "\n",
    "#perform cross-validation and store the F1 scores for each fold\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    fold_f1_scores = f1_scorer(y_test, y_pred)\n",
    "    all_f1_scores.append(fold_f1_scores)\n",
    "\n",
    "#convert the list of arrays to a numpy array\n",
    "f1_scores = np.array(all_f1_scores)\n",
    "\n",
    "#save the F1 scores for each class separately\n",
    "class_names = [f'Class_{i}' for i in range(f1_scores.shape[1])]\n",
    "data = {class_name: f1_scores[:, i] for i, class_name in enumerate(class_names)}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "#save the DataFrame to a CSV file\n",
    "df.to_csv('marPRISM_f1_scores_byClass.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a list of values of k to try\n",
    "k_values = [6]\n",
    "\n",
    "#define a list of train sizes to iterate through\n",
    "train_sizes = np.arange(0.05, 1, 0.05)  # Add or modify train sizes as needed\n",
    "\n",
    "#initialize dictionaries to store the results for each k and train size\n",
    "results = {}\n",
    "\n",
    "#load your dataset and create X, y\n",
    "\n",
    "#{'gamma': 0.0, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 1000, 'reg_lambda': 1.0} \n",
    "clf = XGBClassifier(gamma=0.0, learning_rate=0.1, max_depth=3, n_estimators=1000, reg_lambda=1.0)\n",
    "\n",
    "#loop through different values of k\n",
    "for k in k_values:\n",
    "    print(f\"\\nNumber of splits (k): {k}\")\n",
    "    \n",
    "    #initialize dictionary to store results for this k\n",
    "    k_results = {}\n",
    "\n",
    "    #loop through different train sizes\n",
    "    for train_size in train_sizes:\n",
    "        print(f\"Train size: {train_size}\")\n",
    "        \n",
    "        #initialize dictionary to store results for this train size\n",
    "        k_results[train_size] = {}\n",
    "\n",
    "        #initialize StratifiedShuffleSplit cross-validator with the current train_size\n",
    "        kf = StratifiedShuffleSplit(n_splits=k, train_size=train_size, random_state=7)\n",
    "\n",
    "        #perform cross-validation and store the results\n",
    "        for train_index, test_index in kf.split(X, y):\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "            #create and fit the XGBoost classifier\n",
    "            clf.fit(X_train, y_train)\n",
    "\n",
    "            #predict on the test set\n",
    "            y_pred = clf.predict(X_test)\n",
    "\n",
    "            #calculate F1 scores by class\n",
    "            class_f1_scores = f1_score(y_test, y_pred, average=None)\n",
    "            for i, score in enumerate(class_f1_scores):\n",
    "                key = f'class_{i}_f1'\n",
    "                if key not in k_results[train_size]:\n",
    "                    k_results[train_size][key] = []\n",
    "                k_results[train_size][key].append(score)\n",
    "\n",
    "        #calculate mean and standard error of the F1 scores for this train size\n",
    "        for key in k_results[train_size]:\n",
    "            if key.startswith('class_'):\n",
    "                scores = np.array(k_results[train_size][key])\n",
    "                k_results[train_size][key] = {'mean': np.mean(scores), 'se': stats.sem(scores)}\n",
    "\n",
    "    #store the results for this k in the overall results dictionary\n",
    "    results[k] = k_results\n",
    "\n",
    "#convert the results to a DataFrame for easier analysis and visualization\n",
    "results_df = pd.DataFrame.from_dict({(i, j): results[i][j] for i in results.keys() for j in results[i].keys()},\n",
    "                                     orient='index')\n",
    "\n",
    "#save the results to a CSV file\n",
    "results_df.to_csv('marPRISM_k_train_size_vs_f1_score_by_class.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
