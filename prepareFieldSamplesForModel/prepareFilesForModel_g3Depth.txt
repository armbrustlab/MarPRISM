###g3 depth

##translate and get longest amino acid reading frame of contigs

##the following code for getting the six-frame translations is from:
##https://github.com/armbrustlab/NPac_euk_gene_catalog/blob/main/translation_frame_selection.sh

##the following six-frame translation code was conducted on the assemblies 

#define the translation function:
function translate_6tr {
	# unzip while inserting study/sample prefix on defline:
	echo "Gunzipping raw and adding prefix to ${PREFIX}"
	# transeq does not work with zipped data in bulk format, so we need to unzip first:
	# Adding DEFLINE_PREFIX while unzipping creates a contig_id unique to this project and sample
	gunzip -c ${INPUT_FASTA} | sed "s/>/>${DEFLINE_PREFIX}_/g" >> 6tr/${PREFIX}.Trinity.fasta
	echo "Translating ${PREFIX}"
	# Translate the unzipped INPUT_FASTA in six frames
	transeq -auto -sformat pearson -frame 6 -sequence 6tr/${PREFIX}.Trinity.fasta -outseq 6tr/${PREFIX}.Trinity.6tr.fasta
	# remove the intermediate unzipped fasta if the 6tr is successfully created:
	if [ -f 6tr/${PREFIX}.Trinity.6tr.fasta ]; then rm 6tr/${PREFIX}.Trinity.fasta; fi
	# For most downstream processes you'll want to compress these data:
	echo "Compressing ${PREFIX}"
	gzip 6tr/${PREFIX}.Trinity.6tr.fasta
}

#get list of assemblies to translate
egthomas@grazer:~/g3Depth$ ls *Trinity.fasta.gz > samples.txt
egthomas@grazer:~/g3Depth$ sed -i 's/.Trinity.fasta.gz//g' samples.txt

#I downloaded this script (keep_longest_frame.py3) which selects the longest translation 
#to keep from here:
#https://github.com/armbrustlab/NPac_euk_gene_catalog/blob/main/keep_longest_frame.py3

#function to get longest reading frame per contig for 6 frame translation
egthomas@grazer:~/g3Depth$ for SAMPLE in $(cat sampleGroups); do
> # Create a defline prefix for the contig defline using G2_PA_DCM and ${SAMPLE} used to create a unique ID
> DEFLINE_PREFIX=${SAMPLE}
> PREFIX=${SAMPLE} # This is for sample names and may be same or different to DEFLINE_PREFIX depending on your file structure
> INPUT_FASTA=${PREFIX}.Trinity.fasta.gz
> translate_6tr
> # This calls a python scrcipt to select and output the frame among each of
> # the six-frame translations that has the longest predicted coding sequence# the -l flag defines a minimum amino acid length for output.
> # the -l flag defines a minimum amino acid length for output.
> #~/g2DCM/keep_longest_frame.py3 -l 100 ${PREFIX}.Trinity.6tr.fasta.gz
> # The python script outputs an unzipped fasta file: ${PREFIX}.Trinity.6tr.bf100.fasta
> # Compress this output file:
> #gzip ${PREFIX}.Trinity.6tr.bf100.fasta
> done

#gets the longest reading frame for each contig
#-l 100 sets the minimum contig length as 100
for file in *fasta; do python3 ~/g2DCM/keep_longest_frame.py3 -l 100 $file; done 

#concatenate longest reading frame across all contigs, samples
egthomas@grazer:~/g3Depth/6tr$ for f in *bf100.fasta; do (cat "${f}"; echo) >> G3_depth_assembledReads.fasta; done

###cluster longest reading frame sequences at 99% identity

#create databse before clustering
egthomas@grazer:~/g3Depth/6tr$ ~/mmseqs/bin/mmseqs createdb G3_depth_assembledReads.fasta G3_depth_assembledReads.db
createdb G3_depth_assembledReads.fasta G3_depth_assembledReads.db

#cluster longest reading frame sequences at 99% identity
egthomas@grazer:~/g3Depth/6tr$ MIN_SEQ_ID=0.99
egthomas@grazer:~/g3Depth/6tr$ mkdir tmp/
egthomas@grazer:~/g3Depth/6tr$ function run_linclust {
~/mmseqs/bin/mmseqs linclust G3_depth_assembledReads.db G3_depth_assembledReads.clusters.db tmp --min-seq-id ${MIN_SEQ_ID}
~/mmseqs/bin/mmseqs result2repseq G3_depth_assembledReads.db G3_depth_assembledReads.clusters.db G3_depth_assembledReads.clusters.rep
~/mmseqs/bin/mmseqs result2flat G3_depth_assembledReads.db G3_depth_assembledReads.db G3_depth_assembledReads.clusters.rep G3_depth_assembledReads.id99.fasta --use-fasta-header
}
egthomas@grazer:~/g3Depth/6tr$ run_linclust

egthomas@grazer:~/g3Depth/6tr$ cp G3_depth_assembledReads.id99.fasta /mnt/nfs/projects/gradients-metat/G3/g3_lightdarkdepth_pa_metat/depth/assemblies/clustered/

###get nucleotide sequences for the longest amino acid reading frames that 
###are cluster representatives

#add file/sample name to each nucleotide contig name 
egthomas@grazer:/scratch/g3Depth$ for f in *.Trinity.fasta; do sed -i "s/^>/>${f}_/" "$f"; done

#concatenate nucleotide assemblies
egthomas@grazer:/scratch/g3Depth$ cat *fasta > g3DepthAssemblies.fasta

#get longest reading frame names for cluster representative contigs
egthomas@grazer:~/g3Depth/6tr$ grep ">" G3_depth_assembledReads.id99.fasta > aminoAcidClusterSequences

R

#loads longest reading frame names for cluster representative contigs
> aa <- read.table("aminoAcidClusterSequences")

#makes variable for nucleotide id of cluster representatives
> aa <- aa %>% mutate(nt_id = str_replace(V1, ">", ""))
> aa <- aa %>% mutate(nt_id = str_replace(nt_id, "_[0-9]{1,}$", ""))

#writes nucleotide ids of longest reading frame names for cluster representative contigs to csv file
> aa %>% distinct(nt_id) %>% write.table("nucleotideClusterSequences", quote = FALSE, row.names = FALSE, col.names = FALSE)

#gets rid of .Trinity.fasta in contig names in nucleotide assemblly file 
egthomas@grazer:/scratch/g3Depth$ sed 's,.Trinity.fasta,,g' -i g3DepthAssemblies.fasta

#get nucleotide sequences of longest reading frame cluster representatives
egthomas@grazer:/scratch/g3Depth$ seqtk subseq g3DepthAssemblies.fasta ~/g3Depth/6tr/nucleotideClusterSequences > nucleotideClusterSequences.fasta

egthomas@grazer:/scratch/g3Depth$ wc -l ~/g3Depth/6tr/nucleotideClusterSequences
32307804 /mnt/nfs/home/egthomas/g3Depth/6tr/nucleotideClusterSequences
egthomas@grazer:/scratch/g3Depth$ grep -c ">" nucleotideClusterSequences.fasta
32307804

egthomas@grazer:/scratch/g3Depth$ rm *Trinity.fasta


###trim reads

#make function for concatenating different sequencing lanes from the same sample, paired end direction
egthomas@grazer:/mnt/nfs/projects/gradients-metat/G3/g3_lightdarkdepth_pa_metat/raw_fastq$ function concat {
cat ${SAMPLE}_L001_R1_001.fastq.gz ${SAMPLE}_L002_R1_001.fastq.gz > ~/g3Depth/trimmedReads/${SAMPLE}_R1.fastq.gz
cat ${SAMPLE}_L001_R2_001.fastq.gz ${SAMPLE}_L002_R2_001.fastq.gz > ~/g3Depth/trimmedReads/${SAMPLE}_R2.fastq.gz 
}

#point to list of samples
egthomas@grazer:/mnt/nfs/projects/gradients-metat/G3/g3_lightdarkdepth_pa_metat/raw_fastq$ SAMPLE_LIST_C="/mnt/nfs/home/egthomas/g3Depth/sampleList"

#concatenate different sequencing lanes of raw reads
egthomas@grazer:/mnt/nfs/projects/gradients-metat/G3/g3_lightdarkdepth_pa_metat/raw_fastq$ for SAMPLE in $(cat ${SAMPLE_LIST_C}); do concat; done

##trim raw reads
RAWDIR="/mnt/nfs/projects/gradients-metat/G3/g3_lightdarkdepth_pa_metat/raw_fastq"
#METADATA="sample_metadata.csv"
ADAPTERS="/mnt/nfs/projects/armbrust-metat/gradients3/g3_station_ns_metat/TruSeq2-PE.fa"
THREADS=48
IMAGE="/mnt/nfs/projects/armbrust-metat/workflow_images/fastq_preprocess/fastq-preprocess.sif"
MOUNT_DIR=$(pwd)

ADAPTERS="/mnt/nfs/projects/armbrust-metat/gradients3/g3_station_ns_metat/TruSeq2-PE.fa"

egthomas@grazer:~/g3Depth/trimmedReads$ cp /mnt/nfs/projects/armbrust-metat/gradients3/g3_station_ns_metat/TruSeq2-PE.fa .
  
for fw_reads in `ls *R1.fastq.gz`; do
        rv_reads=${fw_reads/R1/R2}
        # sample is labeled by SampleID in sample_metadata.csv file
        sample=$(basename $fw_reads)
        sample=${sample/_R1.fastq.gz/}
        # skip over any that have already been trimmed
        if [[ -e qc_data/logs/${sample}.trimmomatic.log ]] && \
            grep -q 'Completed successfully' qc_data/logs/${sample}.trimmomatic.log; then 
                printf "Skipping sample %s: trimming already complete" ${sample}
        else
            # log the time
            echo "Start time: " $(date) >> qc_data/logs/${sample}.trimmomatic.log 2>&1
            # java -jar Trimmomatic-0.39/trimmomatic-0.39.jar PE -threads 10 ${fw_reads} ${rv_reads} \
            # docker run -v $(pwd):/home fastq-preprocess \
            singularity exec --bind $(pwd) /mnt/nfs/projects/armbrust-metat/workflow_images/fastq_preprocess/fastq-preprocess.sif \
                trimmomatic PE -threads 10 ${fw_reads} ${rv_reads} \
                qc_data/${sample}.R1.fastq \
                qc_data/${sample}.unpaired.R1.fastq \
                qc_data/${sample}.R2.fastq \
                qc_data/${sample}.unpaired.R2.fastq \
                ILLUMINACLIP:TruSeq2-PE.fa:2:30:10:1:true \
                MAXINFO:135:0.5 LEADING:3 TRAILING:3 MINLEN:60 AVGQUAL:20 >> qc_data/logs/${sample}.trimmomatic.log 2>&1
            # log the time
            echo "End time: " $(date) >> qc_data/logs/${sample}.trimmomatic.log 2>&1
        fi
    done

#run fastqc on pre-trim fastq files
egthomas@grazer:~/g3Depth/trimmedReads$ singularity exec --bind $(pwd) /mnt/nfs/projects/armbrust-metat/workflow_images/fastq_preprocess/fastq-preprocess.sif \
        fastqc `ls *.fastq.gz` -t 2 -o qc_data/reports/pre-trim
     
#run fastqc on post-trim fastq files
egthomas@grazer:~/g3Depth/trimmedReads$ singularity exec --bind $(pwd) /mnt/nfs/projects/armbrust-metat/workflow_images/fastq_preprocess/fastq-preprocess.sif \
	fastqc `ls qc_data/*.fastq` -t 2 -o qc_data/reports/post-trim


##Combine QC reports with MultiQC #
# compile pre-trim report with multiqc
# docker run -v ${MOUNT_DIR}:/home fastq-preprocess \
#     multiqc qc_data/reports/pre-trim -o qc_data/reports/pre-trim
egthomas@grazer:~/g3Depth/trimmedReads$ singularity exec --bind $(pwd) /mnt/nfs/projects/armbrust-metat/workflow_images/fastq_preprocess/fastq-preprocess.sif \
multiqc qc_data/reports/pre-trim -o qc_data/reports/pre-trim
# compile post-trim report with multiqc
# docker run -v $(pwd):/home fastq-preprocess \
#     multiqc qc_data/reports/post-trim -o qc_data/reports/post-trim
egthomas@grazer:~/g3Depth/trimmedReads$ singularity exec --bind $(pwd) /mnt/nfs/projects/armbrust-metat/workflow_images/fastq_preprocess/fastq-preprocess.sif \
multiqc qc_data/reports/post-trim -o qc_data/reports/post-trim

##taxonomically annotate transcripts

#diamond blast last common ancestor annotate contigs using marferret and 
#marmicrodb
egthomas@grazer:~/g3Depth$ diamond blastp --no-unlink -t ~/ -b 100 -c 1 -p 32 -d /mnt/nfs/projects/marferret/v1/data/marmicrodb/dmnd/MarFERReT.v1.1.MMDB.combined.dmnd -e 1e-5 --top 10 -f 102 -q 6tr/G3_depth_assembledReads.id99.fasta -o tax/G3_depth_assembledReads.id99.vs_MarFERReT.v1.1.MMDB.combined_noOutliers.lca.tab_fall2023

##functionally annotate contigs with pfam

#functionally annotate longest reading frame clustered amino acid sequences of contigs with pfam database
#this code is shortened from
#https://github.com/armbrustlab/marine_eukaryote_sequence_database/blob/main/pfam_annotation.sh
egthomas@grazer:~/g3Depth/6tr$ hmmsearch --cut_tc --domtblout ../func/G3_depth_assembledReads.id99.PFAM34.0.domtblout.tab /mnt/nfs/projects/ryan/PFAM/Pfam_34.0/Pfam-A.hmm G3_depth_assembledReads.id99.fasta

egthomas@grazer:~/g3Depth/func$ cp G3_depth_assembledReads.id99.PFAM34.0.domtblout.tab /mnt/nfs/projects/gradients-metat/G3/g3_lightdarkdepth_pa_metat/depth/assemblies/func/


##find best pfam for each contig

R

#loads the concatenated pfam results for the g3 depth polyA metaT contigs
> res <- read.table("G3_depth_assembledReads.id99.PFAM34.0.domtblout.tab")

> str(res$V8)
 num [1:15299212] 99.5 99.5 99.5 81.3 81.3 74.6 74.6 73.3 73.3 72.8 ...
 
> str(res$V7)
 num [1:15299212] 1.3e-25 1.3e-25 1.3e-25 6.0e-20 6.0e-20 ...

#get only pfam annotations with evalue 10^-5
> res <- res %>% filter(V7 < 10^(-5))

> #for contig reading frame, get the pfam with the highest score
> best_pfam_dat <- res %>% group_by(V1) %>% slice(which.max(V8))

#make nucleotide id variable
> best_pfam_dat <- best_pfam_dat %>% mutate(nt_id = str_replace(V1, "_[0-9]{1,}$", ""))

> best_pfam_dat <- best_pfam_dat %>% ungroup()

#there are more rows than nt_id because some reading frames have the same length 
#and same value for V8
> best_pfam_dat %>% distinct(V1) %>% nrow()
[1] 7592941
> best_pfam_dat %>% distinct(nt_id) %>% nrow()
[1] 7585157

#get only one pfam annotation per nucleotide id
> best_pfam_dat <- best_pfam_dat %>% ungroup() %>% group_by(nt_id) %>% slice(1)

> best_pfam_dat %>% ungroup() %>% nrow()
[1] 7585157

#writes best pfam for each nucleotide to a csv
> best_pfam_dat %>% write_csv("G3_depth_pfamAnnotations_bestPfam.tab")


#nucleotide cluster sequences were indexed and used for 
##unstranded kallisto mapping by Sacha Coesel
##Sacha mapped unstranded because assembly was created unstranded
egthomas@guppy:/scratch/elaina$ cp /mnt/nfs/projects/armbrust-metat/gradients3/g3_depth_pa_metat/kallisto/unstranded/G3PA_depth.raw.est_counts_unstranded.csv.gz .
egthomas@guppy:/scratch/elaina$ gunzip *
egthomas@guppy:/scratch/elaina$ mv G3PA_depth.raw.est_counts_unstranded.csv ~/g3Depth/mappedShortReads/

R

#read in the concatenated g3 depth polyA metaT mapping abundance file
> dat <- read_csv("G3PA_depth.raw.est_counts_unstranded.csv")

#gather counts data
> dat <- dat %>% gather(G3PA.depth.S8C2.DCM.B:G3PA.depth.S8C2.75m.B, key = "sample", value = "est_counts")

#gets rid of contigs with 0 mapped reads
> dat <- dat %>% filter(est_counts != 0)

#load taxa annotation file that was produced with database including 
#updated marferret and marmicrodb
> tax <- read.table("~/g3Depth/tax/G3_depth_assembledReads.id99.vs_MarFERReT.v1.1.MMDB.combined_noOutliers.lca.tab_fall2023")

> str(tax)
'data.frame':   35165701 obs. of  3 variables:
 $ V1: chr  "S6C7_75m_B_TRINITY_DN1562619_c0_g1_i1_3" "S6C7_75m_B_TRINITY_DN1526198_c0_g1_i1_3" "S6C7_75m_B_TRINITY_DN1538274_c1_g1_i3_1" "S6C7_75m_B_TRINITY_DN1538257_c2_g1_i1_3" ...
 $ V2: int  69332 33656 0 632150 33630 0 412157 156230 0 0 ...
 $ V3: num  1.2e-10 8.8e-18 0.0 6.1e-23 2.2e-47 ...

#gets rid of rows that have an NA taxa annotation (some unannotated sequences are
#labeled as taxa id 0)
> tax <- tax %>% filter(!is.na(V2))
> tax <- tax %>% filter(V2 != 0)

#make nucleotide id variable
> tax <- tax %>% mutate(nt_id = str_replace(V1, "_[0-9]{1,}$", ""))

#get best taxa annotation per nucleotide contig id
> tax_best <- tax %>% group_by(nt_id) %>% arrange(V3) %>% slice(1)

> tax_best %>% nrow()
[1] 18978802
> tax_best %>% distinct(nt_id) %>% nrow()
[1] 18978802

> tax_best %>% write_csv("~/g3Depth/tax/G3_depth_assembledReads.id99.vs_MarFERReT.v1.1.MMDB.combined_noOutliers.lca.tab_fall2023_best.csv")

#add taxa annotations of contigs to the number of reads mapped to contig per sample
> nrow(dat_tax)
[1] 159214039
> dat_tax <- dat %>% left_join(tax_best, by = c("target_id" = "nt_id"))
> nrow(dat_tax)
[1] 159214039

#get rid of contigs without taxa annotations
> dat_tax_noNA <- dat_tax %>% filter(!is.na(V2))

> dat_tax_noNA %>% write_csv("~/g3Depth/mappedShortReads/G3_depth_mappedReadAbundance_noZeroes.csv")

#loads best pfam annotation for each nucleotide contig id
> best_pfam_dat <- read_csv("~/g3Depth/func/G3_depth_pfamAnnotations_bestPfam.tab")
 
> str(dat_tax_noNA)
tibble [97,751,527 × 8] (S3: tbl_df/tbl/data.frame)
 $ ...1      : num [1:97751527] 5 9 39 57 60 63 66 67 82 84 ...
 $ target_id : chr [1:97751527] "S4C6_B_15m_TRINITY_DN3635074_c0_g1_i1" "S4C6_B_15m_TRINITY_DN3635082_c0_g1_i1" "S4C6_B_15m_TRINITY_DN3635104_c0_g1_i1" "S4C6_B_15m_TRINITY_DN3635059_c0_g1_i1" ...
 $ length    : num [1:97751527] 402 712 354 395 527 553 367 394 531 304 ...
 $ sample    : chr [1:97751527] "G3PA.depth.S8C2.DCM.B" "G3PA.depth.S8C2.DCM.B" "G3PA.depth.S8C2.DCM.B" "G3PA.depth.S8C2.DCM.B" ...
 $ est_counts: num [1:97751527] 1 4 1 2 1 ...
 $ V1        : chr [1:97751527] "S4C6_B_15m_TRINITY_DN3635074_c0_g1_i1_6" "S4C6_B_15m_TRINITY_DN3635082_c0_g1_i1_2" "S4C6_B_15m_TRINITY_DN3635104_c0_g1_i1_6" "S4C6_B_15m_TRINITY_DN3635059_c0_g1_i1_1" ...
 $ V2        : int [1:97751527] 2698737 2836 2864 407301 2759 49237 407301 2759 2864 109239 ...
 $ V3        : num [1:97751527] 2.1e-07 7.8e-61 6.2e-19 6.5e-41 9.0e-14 ...

#load taxa names corresponding to each taxa id in marferret
> name <- read_csv("/mnt/nfs/projects/marferret/v1/data/MarFERReT.v1.taxa.csv")

#get rid of unnecessary variables
> name <- name %>% select(1:4)

#add taxa names to counts data
> nrow(merged2)
[1] 97751527
> merged2 <- merged2 %>% left_join(name, by = c("V2" = "tax_id"))
> nrow(merged2)
[1] 97751527

##calculate transcripts per million
> merged2 <- dat_tax_noNA %>% mutate(length_kb = length/1000)
> merged2 <- merged2 %>% mutate(est_counts_div_length_kb = est_counts/length_kb)

> mCounts <- merged2 %>% group_by(sample, tax_name) %>% summarize(mCount = sum(est_counts_div_length_kb)/1e6)
`summarise()` has grouped output by 'sample'. You can override using the `.groups` argument.

> nrow(merged2)
[1] 97751527
> merged2 <- merged2 %>% left_join(mCounts, by = c("sample", "tax_name"))
> nrow(merged2)
[1] 97751527

> merged2 <- merged2 %>% mutate(tpm = est_counts_div_length_kb/mCount)

#add best pfam annotation to each nucleotide contig id
> nrow(merged2)
[1] 54235686
> merged2_pfam <- merged2 %>% left_join(best_pfam_dat %>% distinct(V1, V4, V5), by = c("V1"))
> nrow(merged2_pfam)
[1] 54235686

#load core transcribed genes (CTGS) identified 
#from 10.1038/s41597-023-02842-4
> core <- read_csv("~/MarFERReT.v1.core_genes.csv")

#get just ctgs grouped at level of eukaryotes
> core <- core %>% filter(lineage == "Eukaryota")

#make a dataframe to count the number of ctgs in each species bin by sample
> pfamSummary <- merged2_pfam %>% semi_join(core, by = c("V5" = "pfam_id"))

#calculate number of ctgs in each species bin by sample
> pfamSummary <- pfamSummary %>% group_by(sample, V2, tax_name) %>% distinct(V5) %>% summarize(numCorePfams = n())
`summarise()` has grouped output by 'sample', 'V2'. You can override using the `.groups` argument.

#there are a total of 605 eukaryote ctgs
> core %>% distinct(pfam_id) %>% summarize(n = n())
# A tibble: 1 × 1
      n
  <int>
1   605

#calculate proportion of ctgs in each species bin by sample
> pfamSummary <- pfamSummary %>% ungroup() %>% mutate(propCorePfams = numCorePfams/605)

> pfamSummary %>% write_csv("~/g3Depth/corePfamSummary.csv")

#get species bins/samples that have at least 70% of ctgs detected
> pfamSummary <- pfamSummary %>% filter(propCorePfams >= .7)

#gets just the counts for taxa and sample pairs that have at least 70% ctgs detected
> merged2_pfam <- merged2_pfam %>% semi_join(pfamSummary, by = c("V2", "sample"))

#for each taxa, sample, and pfam combination, get the total tpm
> g3Depth_summary <- merged2_pfam %>% ungroup() %>% dplyr::group_by(sample, V2, tax_name, V5) %>% dplyr::summarize(tpm = sum(tpm))
`summarise()` has grouped output by 'sample', 'V2', 'tax_name'. You can override using the `.groups` argument.

#ungroup counts summary data
> g3Depth_summary <- g3Depth_summary %>% ungroup()

#after calculating tpm, get rid of contigs without pfam annotations
> g3Depth_summary <- g3Depth_summary %>% filter(!is.na(V5))

#name pfam variable
> colnames(g3Depth_summary)[4] <- "pfam"
  
#make pfam variable without digits after decimal
> g3Depth_summary <- g3Depth_summary %>% mutate(shortPfam = str_replace(pfam, "\\..{1,}$", ""))

#get rid of unnecessary variables
> merg <- g3Depth_summary %>% select(sample, tax_name, shortPfam, tpm)

#load feature pfams
> pfam <- read_csv("~/Extracted_Pfams_noOutliers_newContaminationMetric_xg.csv")

#rename pfam variable
> colnames(merg)[3] <- "pfam"

#get feature pfams missing from counts data
> miss <- pfam %>% anti_join(merg, by = c("pfam"))

#add missing feature pfams to counts data with tpm 0 
> miss <- miss %>% mutate(sample = "G3PA.depth.S4C6.15m.A", tax_name = "Tripos fusus")

> merg <- merg %>% bind_rows(miss)

#spread data, filling in missing pfams with tpm 0
> merg <- merg %>% spread(key = pfam, value = tpm, fill = 0)

> merg %>% select(sample:tax_name) %>% write_csv("~/g3Depth/g3Depth_tpm_updatedMarferret_marmicroDb2023_sampleTaxa_noOutliers_fixedTPM_fall2023.csv")
> merg %>% select(-c(sample:tax_name)) %>% write_csv("~/g3Depth/g3Depth_tpm_updatedMarferret_marmicroDb2023_noOutliers_fixedTPM_fall2023.csv")

#move files into permanent shared folder on grazer
egthomas@grazer:~/g3Depth/6tr$ mv *6tr.fasta /mnt/nfs/projects/gradients-metat/G3/g3_lightdarkdepth_pa_metat/depth/PE/6tr/
egthomas@grazer:~/g3Depth/6tr$ mv *bf100.fasta /mnt/nfs/projects/gradients-metat/G3/g3_lightdarkdepth_pa_metat/depth/PE/6tr/bf/
egthomas@grazer:~/g3Depth$ mv *fasta.gz /mnt/nfs/projects/gradients-metat/G3/g3_lightdarkdepth_pa_metat/depth/assemblies/
egthomas@grazer:~/g3Depth/trimmedReads/qc_data$ mv * /mnt/nfs/projects/gradients-metat/G3/g3_lightdarkdepth_pa_metat/depth/PE/

