{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedShuffleSplit\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "from xgboost import XGBClassifier\n",
    "from scipy import stats\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MarPRISM\n",
    "#xgboost model, xgboost features\n",
    "#training data without contamination or low sequence abundance \n",
    "#updated software"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#load training data and labels\n",
    "train = pd.read_csv('../trainingDataMarPRISM.csv')\n",
    "\n",
    "#get just the TPM values not the MMETSP entry IDs and trophic mode labels\n",
    "trainData = train.iloc[:, 2:]\n",
    "\n",
    "#load feature Pfams for model\n",
    "features = pd.read_csv('../MarPRISM_featurePfams.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract pfam column from features dataframe\n",
    "features = features['pfam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to encode trophic labels as numbers (0,1,2)\n",
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get just data for the feature Pfams from training data\n",
    "trainData = trainData[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign feature matrix and target vector\n",
    "X, y = trainData, le.fit_transform(train['Trophic mode'])\n",
    "#X: Feature matrix (independent variables) from the DataFrame trainData\n",
    "#y: Target vector (dependent variable), where the 'Trophic mode' column is label-encoded using `LabelEncoder`.\n",
    "\n",
    "#initialize a MinMaxScaler instance\n",
    "scaler = MinMaxScaler()\n",
    "#minMaxScaler scales features to a specified range, typically [0, 1], which can improve the performance of machine learning models\n",
    "\n",
    "#scale feature matrix\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the dataset into training and testing subsets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,           # Features (input variables)\n",
    "    y,           # Target variable (output/labels)\n",
    "    test_size=0.4, # Proportion of the dataset to include in the test split (40%)\n",
    "    random_state=0 # Random seed for reproducibility of the split\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(7)\n",
    "\n",
    "#{'gamma': 0.0, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 1000, 'reg_lambda': 1.0} \n",
    "clf = XGBClassifier(gamma=0.0, learning_rate=0.1, max_depth=3, n_estimators=1000, reg_lambda=1.0)\n",
    "\n",
    "#define a custom scoring function for F1 score\n",
    "f1_scorer = make_scorer(f1_score, average='weighted')\n",
    "\n",
    "#create a StratifiedShuffleSplit cross-validator\n",
    "kf = StratifiedShuffleSplit(n_splits=6, random_state=7)\n",
    "\n",
    "#perform cross-validation and store the F1 scores\n",
    "f1_scores = cross_val_score(clf, X, y, cv=kf, scoring=f1_scorer)\n",
    "\n",
    "#save F1 scores\n",
    "data = {'F1_Scores': f1_scores}\n",
    "\n",
    "#calculate mean and standard error of F1 scores\n",
    "mean_f1 = data['F1_Scores'].mean()\n",
    "std_error_f1 = data['F1_Scores'].std(ddof=1) / np.sqrt(len(data['F1_Scores']))\n",
    "\n",
    "#create a new DataFrame with the results\n",
    "df = pd.DataFrame({\n",
    "    'Model': ['MarPRISM (xgboost model, xgboost feature Pfams, training data without contamination and low sequence abundance, updated software)'],\n",
    "    'Mean_F1_Score': [mean_f1],\n",
    "    'Std_Error_F1_Score': [std_error_f1]\n",
    "})\n",
    "\n",
    "#define output csv path\n",
    "csv_file_path = 'model_overall_f1_scores.csv'\n",
    "\n",
    "#check if the file already exists\n",
    "if os.path.exists(csv_file_path):\n",
    "    #append to the file without writing the header\n",
    "    df.to_csv(csv_file_path, mode='a', header=False, index=False)\n",
    "else:\n",
    "    #write the DataFrame as a new file with header\n",
    "    df.to_csv(csv_file_path, mode='w', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(7)\n",
    "\n",
    "#{'gamma': 0.0, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 1000, 'reg_lambda': 1.0} #\n",
    "clf = XGBClassifier(gamma=0.0, learning_rate=0.1, max_depth=3, n_estimators=1000, reg_lambda=1.0)\n",
    "\n",
    "#define a custom scoring function for F1 score\n",
    "def f1_scorer(y_true, y_pred):\n",
    "    return f1_score(y_true, y_pred, average=None)\n",
    "\n",
    "#create a StratifiedShuffleSplit cross-validator\n",
    "kf = StratifiedShuffleSplit(n_splits=6, random_state=7)\n",
    "\n",
    "#initialize an empty list to store F1 scores for each class\n",
    "all_f1_scores = []\n",
    "\n",
    "#perform cross-validation and store the F1 scores for each fold\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    fold_f1_scores = f1_scorer(y_test, y_pred)\n",
    "    all_f1_scores.append(fold_f1_scores)\n",
    "\n",
    "#convert the list of arrays to a numpy array\n",
    "f1_scores = np.array(all_f1_scores)\n",
    "\n",
    "#save the F1 scores for each trophic mode separately\n",
    "trophic_names = [f'Trophic_{i}' for i in range(f1_scores.shape[1])]\n",
    "data = {trophic_name: f1_scores[:, i] for i, trophic_name in enumerate(trophic_names)}\n",
    "\n",
    "#prepare a list to store results\n",
    "results = []\n",
    "\n",
    "#loop through the data dictionary and calculate mean and standard error for each class\n",
    "for class_name, f1_scores in data.items():\n",
    "    mean_f1 = np.mean(f1_scores)\n",
    "    std_error_f1 = np.std(f1_scores, ddof=1) / np.sqrt(len(f1_scores))  # Standard error\n",
    "    results.append({\n",
    "        'Model': 'MarPRISM (xgboost model, xgboost feature Pfams, training data without contamination and low sequence abundance, updated software)',\n",
    "        'Trophic mode': class_name,\n",
    "        'Mean_F1_Score': mean_f1,\n",
    "        'Std_Error_F1_Score': std_error_f1\n",
    "    })\n",
    "\n",
    "#create a DataFrame from the results\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "#replace Trophic values with desired labels\n",
    "df['Trophic mode'] = df['Trophic mode'].replace({'Trophic_0': 'Het', 'Trophic_1': 'Mix', 'Trophic_2': 'Phot'})\n",
    "\n",
    "#define output csv path\n",
    "csv_file_path = 'models_byClass_f1_scores.csv'\n",
    "\n",
    "#check if the file already exists\n",
    "if os.path.exists(csv_file_path):\n",
    "    #append to the file without writing the header\n",
    "    df.to_csv(csv_file_path, mode='a', header=False, index=False)\n",
    "else:\n",
    "    #write the DataFrame as a new file with header\n",
    "    df.to_csv(csv_file_path, mode='w', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of splits (k): 6\n",
      "Train size: 0.05\n",
      "Train size: 0.1\n",
      "Train size: 0.15000000000000002\n",
      "Train size: 0.2\n",
      "Train size: 0.25\n",
      "Train size: 0.3\n",
      "Train size: 0.35000000000000003\n",
      "Train size: 0.4\n",
      "Train size: 0.45\n",
      "Train size: 0.5\n",
      "Train size: 0.55\n",
      "Train size: 0.6000000000000001\n",
      "Train size: 0.6500000000000001\n",
      "Train size: 0.7000000000000001\n",
      "Train size: 0.7500000000000001\n",
      "Train size: 0.8\n",
      "Train size: 0.8500000000000001\n",
      "Train size: 0.9000000000000001\n",
      "Train size: 0.9500000000000001\n"
     ]
    }
   ],
   "source": [
    "#define a list of values of k to try\n",
    "k_values = [6]\n",
    "\n",
    "#define a list of train sizes to iterate through\n",
    "train_sizes = np.arange(0.05, 1, 0.05)  \n",
    "\n",
    "#initialize an empty list to store results for each k and train size\n",
    "results = [] \n",
    "\n",
    "#initialize the classifier (XGBoost)\n",
    "clf = XGBClassifier(gamma=0.0, learning_rate=0.1, max_depth=3, n_estimators=1000, reg_lambda=1.0)\n",
    "\n",
    "#loop through different values of k\n",
    "for k in k_values:\n",
    "    print(f\"\\nNumber of splits (k): {k}\")\n",
    "    \n",
    "    #loop through different train sizes\n",
    "    for train_size in train_sizes:\n",
    "        print(f\"Train size: {train_size}\")\n",
    "        \n",
    "        #initialize dictionary to store results for this train size\n",
    "        f1_scores_dict = {'k': k, 'train_size': train_size}\n",
    "        \n",
    "        #initialize StratifiedShuffleSplit cross-validator with the current train_size\n",
    "        kf = StratifiedShuffleSplit(n_splits=k, train_size=train_size, random_state=7)\n",
    "\n",
    "        #perform cross-validation and store the results\n",
    "        class_f1_scores_all = {f'class_{i}_f1': [] for i in range(3)}  #3 classes\n",
    "        \n",
    "        for train_index, test_index in kf.split(X, y):\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "            #create and fit the XGBoost classifier\n",
    "            clf.fit(X_train, y_train)\n",
    "\n",
    "            #predict on the test set\n",
    "            y_pred = clf.predict(X_test)\n",
    "\n",
    "            #calculate F1 scores by class\n",
    "            class_f1_scores = f1_score(y_test, y_pred, average=None)\n",
    "\n",
    "            #store the class F1 scores\n",
    "            for i, score in enumerate(class_f1_scores):\n",
    "                class_f1_scores_all[f'class_{i}_f1'].append(score)\n",
    "        \n",
    "        #calculate mean and standard error of the F1 scores for this train size\n",
    "        for key, scores in class_f1_scores_all.items():\n",
    "            mean_score = np.mean(scores)\n",
    "            se_score = stats.sem(scores)\n",
    "            f1_scores_dict[f'{key}_mean'] = mean_score\n",
    "            f1_scores_dict[f'{key}_se'] = se_score\n",
    "        \n",
    "        #append the results for this train size\n",
    "        results.append(f1_scores_dict)\n",
    "\n",
    "#convert the results to a DataFrame for easier analysis and visualization\n",
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove first column with number of folds\n",
    "results_df.drop(results_df.columns[0], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename columns\n",
    "results_df.columns = ['proportion of training data used', 'Het_Mean_F1_Score', 'Het_F1_Std_Error_F1_Score', \n",
    "                      'Mix_Mean_F1_Score', 'Mix_F1_Std_Error_F1_Score', 'Phot_Mean_F1_Score', 'Phot_F1_Std_Error_F1_Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the results of F1 score versus different percentages of \n",
    "#training data used to a CSV file\n",
    "results_df.to_csv('marPRISM_k_train_size_vs_f1_score_by_class.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate confusion matrix summed across folds of cross-validation\n",
    "\n",
    "cumulative_cm = np.zeros((3, 3), dtype=int)  # 3 classes\n",
    "\n",
    "# Redefine and rerun the loop if needed\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    cumulative_cm += cm\n",
    "\n",
    "class_names = ['Het', 'Mix', 'Phot']\n",
    "cm_df = pd.DataFrame(cumulative_cm, index=class_names, columns=class_names)\n",
    "cm_df.to_csv('marPRISM_cumulative_confusion_matrix.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xgboost model, xgboost and random forest features\n",
    "#training data without contamination or low sequence abundance \n",
    "#updated software"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get just the TPM values not the MMETSP entry IDs and trophic mode labels\n",
    "#same training data as above\n",
    "trainData = train.iloc[:, 2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load xgboost and random forest feature Pfams\n",
    "features = pd.read_csv('Extracted_Pfams_contaminationLowSeqsRemoved_xgModel_xgRFFeatures.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract pfam column from features dataframe\n",
    "features = features['pfam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to encode trophic labels as numbers (0,1,2)\n",
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get just data for the feature Pfams from training data\n",
    "trainData = trainData[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign feature matrix and target vector\n",
    "X, y = trainData, le.fit_transform(train['Trophic mode'])\n",
    "#X: Feature matrix (independent variables) from the DataFrame trainData\n",
    "#y: Target vector (dependent variable), where the 'Trophic mode' column is label-encoded using `LabelEncoder`.\n",
    "\n",
    "#initialize a MinMaxScaler instance\n",
    "scaler = MinMaxScaler()\n",
    "#minMaxScaler scales features to a specified range, typically [0, 1], which can improve the performance of machine learning models\n",
    "\n",
    "#scale feature matrix\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the dataset into training and testing subsets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,           # Features (input variables)\n",
    "    y,           # Target variable (output/labels)\n",
    "    test_size=0.4, # Proportion of the dataset to include in the test split (40%)\n",
    "    random_state=0 # Random seed for reproducibility of the split\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(7)\n",
    "\n",
    "#{'gamma': 0.0, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 1000, 'reg_lambda': 1.0} \n",
    "clf = XGBClassifier(gamma=0.0, learning_rate=0.1, max_depth=3, n_estimators=1000, reg_lambda=1.0)\n",
    "\n",
    "#define a custom scoring function for F1 score\n",
    "f1_scorer = make_scorer(f1_score, average='weighted')\n",
    "\n",
    "#create a StratifiedShuffleSplit cross-validator\n",
    "kf = StratifiedShuffleSplit(n_splits=6, random_state=7)\n",
    "\n",
    "#perform cross-validation and store the F1 scores\n",
    "f1_scores = cross_val_score(clf, X, y, cv=kf, scoring=f1_scorer)\n",
    "\n",
    "#save F1 scores\n",
    "data = {'F1_Scores': f1_scores}\n",
    "\n",
    "#calculate mean and standard error of F1 scores\n",
    "mean_f1 = data['F1_Scores'].mean()\n",
    "std_error_f1 = data['F1_Scores'].std(ddof=1) / np.sqrt(len(data['F1_Scores']))\n",
    "\n",
    "#create a new DataFrame with the results\n",
    "df = pd.DataFrame({\n",
    "    'Model': ['xgboost model, xgboost and random forest feature Pfams, training data without contamination and low sequence abundance, updated software'],\n",
    "    'Mean_F1_Score': [mean_f1],\n",
    "    'Std_Error_F1_Score': [std_error_f1]\n",
    "})\n",
    "\n",
    "#define output csv path\n",
    "csv_file_path = 'model_overall_f1_scores.csv'\n",
    "\n",
    "#check if the file already exists\n",
    "if os.path.exists(csv_file_path):\n",
    "    #append to the file without writing the header\n",
    "    df.to_csv(csv_file_path, mode='a', header=False, index=False)\n",
    "else:\n",
    "    #write the DataFrame as a new file with header\n",
    "    df.to_csv(csv_file_path, mode='w', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(7)\n",
    "\n",
    "#{'gamma': 0.0, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 1000, 'reg_lambda': 1.0} #\n",
    "clf = XGBClassifier(gamma=0.0, learning_rate=0.1, max_depth=3, n_estimators=1000, reg_lambda=1.0)\n",
    "\n",
    "#define a custom scoring function for F1 score\n",
    "def f1_scorer(y_true, y_pred):\n",
    "    return f1_score(y_true, y_pred, average=None)\n",
    "\n",
    "#create a StratifiedShuffleSplit cross-validator\n",
    "kf = StratifiedShuffleSplit(n_splits=6, random_state=7)\n",
    "\n",
    "#initialize an empty list to store F1 scores for each class\n",
    "all_f1_scores = []\n",
    "\n",
    "#perform cross-validation and store the F1 scores for each fold\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    fold_f1_scores = f1_scorer(y_test, y_pred)\n",
    "    all_f1_scores.append(fold_f1_scores)\n",
    "\n",
    "#convert the list of arrays to a numpy array\n",
    "f1_scores = np.array(all_f1_scores)\n",
    "\n",
    "#save the F1 scores for each trophic mode separately\n",
    "trophic_names = [f'Trophic_{i}' for i in range(f1_scores.shape[1])]\n",
    "data = {trophic_name: f1_scores[:, i] for i, trophic_name in enumerate(trophic_names)}\n",
    "\n",
    "#prepare a list to store results\n",
    "results = []\n",
    "\n",
    "#loop through the data dictionary and calculate mean and standard error for each class\n",
    "for class_name, f1_scores in data.items():\n",
    "    mean_f1 = np.mean(f1_scores)\n",
    "    std_error_f1 = np.std(f1_scores, ddof=1) / np.sqrt(len(f1_scores))  # Standard error\n",
    "    results.append({\n",
    "        'Model': 'xgboost model, xgboost and random forest feature Pfams, training data without contamination and low sequence abundance, updated software',\n",
    "        'Trophic mode': class_name,\n",
    "        'Mean_F1_Score': mean_f1,\n",
    "        'Std_Error_F1_Score': std_error_f1\n",
    "    })\n",
    "\n",
    "#create a DataFrame from the results\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Replace Trophic values with desired labels\n",
    "df['Trophic mode'] = df['Trophic mode'].replace({'Trophic_0': 'Het', 'Trophic_1': 'Mix', 'Trophic_2': 'Phot'})\n",
    "\n",
    "#define output csv path\n",
    "csv_file_path = 'models_byClass_f1_scores.csv'\n",
    "\n",
    "#check if the file already exists\n",
    "if os.path.exists(csv_file_path):\n",
    "    #append to the file without writing the header\n",
    "    df.to_csv(csv_file_path, mode='a', header=False, index=False)\n",
    "else:\n",
    "    #write the DataFrame as a new file with header\n",
    "    df.to_csv(csv_file_path, mode='w', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random forest model, random forest model feature pfams\n",
    "#training data without contamination or low sequence abundance \n",
    "#updated software"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get just the TPM values not the MMETSP entry IDs and trophic mode labels\n",
    "#same training data as above\n",
    "trainData = train.iloc[:, 2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load random forest feature Pfams\n",
    "features = pd.read_csv('Extracted_Pfams_contaminationLowSeqsRemoved_rfModel_rfFeatures.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract pfam column from features dataframe\n",
    "features = features['pfam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to encode trophic labels as numbers (0,1,2)\n",
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get just data for the feature Pfams from training data\n",
    "trainData = trainData[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign feature matrix and target vector\n",
    "X, y = trainData, le.fit_transform(train['Trophic mode'])\n",
    "#X: Feature matrix (independent variables) from the DataFrame trainData\n",
    "#y: Target vector (dependent variable), where the 'Trophic mode' column is label-encoded using `LabelEncoder`.\n",
    "\n",
    "#initialize a MinMaxScaler instance\n",
    "scaler = MinMaxScaler()\n",
    "#minMaxScaler scales features to a specified range, typically [0, 1], which can improve the performance of machine learning models\n",
    "\n",
    "#scale feature matrix\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the dataset into training and testing subsets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,           # Features (input variables)\n",
    "    y,           # Target variable (output/labels)\n",
    "    test_size=0.4, # Proportion of the dataset to include in the test split (40%)\n",
    "    random_state=0 # Random seed for reproducibility of the split\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(7)\n",
    "\n",
    "#{'max_depth': 1000, 'min_samples_leaf': 5, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 10} \n",
    "clf = RandomForestClassifier(max_depth=1000,min_samples_leaf=5,min_samples_split=2,min_weight_fraction_leaf=0.0,n_estimators=10)\n",
    "\n",
    "#define a custom scoring function for F1 score\n",
    "f1_scorer = make_scorer(f1_score, average='weighted')\n",
    "\n",
    "#create a StratifiedShuffleSplit cross-validator\n",
    "kf = StratifiedShuffleSplit(n_splits=6, random_state=7)\n",
    "\n",
    "#perform cross-validation and store the F1 scores\n",
    "f1_scores = cross_val_score(clf, X, y, cv=kf, scoring=f1_scorer)\n",
    "\n",
    "#save F1 scores\n",
    "data = {'F1_Scores': f1_scores}\n",
    "\n",
    "#calculate mean and standard error of F1 scores\n",
    "mean_f1 = data['F1_Scores'].mean()\n",
    "std_error_f1 = data['F1_Scores'].std(ddof=1) / np.sqrt(len(data['F1_Scores']))\n",
    "\n",
    "#create a new DataFrame with the results\n",
    "df = pd.DataFrame({\n",
    "    'Model': ['random forest model, random forest feature Pfams, training data without contamination and low sequence abundance, updated software'],\n",
    "    'Mean_F1_Score': [mean_f1],\n",
    "    'Std_Error_F1_Score': [std_error_f1]\n",
    "})\n",
    "\n",
    "#define output csv path\n",
    "csv_file_path = 'model_overall_f1_scores.csv'\n",
    "\n",
    "#check if the file already exists\n",
    "if os.path.exists(csv_file_path):\n",
    "    #append to the file without writing the header\n",
    "    df.to_csv(csv_file_path, mode='a', header=False, index=False)\n",
    "else:\n",
    "    #write the DataFrame as a new file with header\n",
    "    df.to_csv(csv_file_path, mode='w', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(7)\n",
    "\n",
    "#{'max_depth': 1000, 'min_samples_leaf': 5, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 10} \n",
    "clf = RandomForestClassifier(max_depth=1000,min_samples_leaf=5,min_samples_split=2,min_weight_fraction_leaf=0.0,n_estimators=10)\n",
    "\n",
    "#define a custom scoring function for F1 score\n",
    "def f1_scorer(y_true, y_pred):\n",
    "    return f1_score(y_true, y_pred, average=None)\n",
    "\n",
    "#create a StratifiedShuffleSplit cross-validator\n",
    "kf = StratifiedShuffleSplit(n_splits=6, random_state=7)\n",
    "\n",
    "#initialize an empty list to store F1 scores for each class\n",
    "all_f1_scores = []\n",
    "\n",
    "#perform cross-validation and store the F1 scores for each fold\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    fold_f1_scores = f1_scorer(y_test, y_pred)\n",
    "    all_f1_scores.append(fold_f1_scores)\n",
    "\n",
    "#convert the list of arrays to a numpy array\n",
    "f1_scores = np.array(all_f1_scores)\n",
    "\n",
    "#save the F1 scores for each trophic mode separately\n",
    "trophic_names = [f'Trophic_{i}' for i in range(f1_scores.shape[1])]\n",
    "data = {trophic_name: f1_scores[:, i] for i, trophic_name in enumerate(trophic_names)}\n",
    "\n",
    "#prepare a list to store results\n",
    "results = []\n",
    "\n",
    "#loop through the data dictionary and calculate mean and standard error for each class\n",
    "for class_name, f1_scores in data.items():\n",
    "    mean_f1 = np.mean(f1_scores)\n",
    "    std_error_f1 = np.std(f1_scores, ddof=1) / np.sqrt(len(f1_scores))  # Standard error\n",
    "    results.append({\n",
    "        'Model': 'random forest model, random forest feature Pfams, training data without contamination and low sequence abundance, updated software',\n",
    "        'Trophic mode': class_name,\n",
    "        'Mean_F1_Score': mean_f1,\n",
    "        'Std_Error_F1_Score': std_error_f1\n",
    "    })\n",
    "\n",
    "#create a DataFrame from the results\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Replace Trophic values with desired labels\n",
    "df['Trophic mode'] = df['Trophic mode'].replace({'Trophic_0': 'Het', 'Trophic_1': 'Mix', 'Trophic_2': 'Phot'})\n",
    "\n",
    "#define output csv path\n",
    "csv_file_path = 'models_byClass_f1_scores.csv'\n",
    "\n",
    "#check if the file already exists\n",
    "if os.path.exists(csv_file_path):\n",
    "    #append to the file without writing the header\n",
    "    df.to_csv(csv_file_path, mode='a', header=False, index=False)\n",
    "else:\n",
    "    #write the DataFrame as a new file with header\n",
    "    df.to_csv(csv_file_path, mode='w', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xgboost model, xgboost features\n",
    "#training data with contamination and low sequence abundance \n",
    "#updated software"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load training data and labels for data that includes \n",
    "#MMETSP transcriptomes with high contamination and \n",
    "#low sequence abundance\n",
    "train = pd.read_csv('trainingData_withContam_withLowSeqs.csv')\n",
    "\n",
    "#get just the TPM values not the MMETSP entry IDs and trophic mode labels\n",
    "trainData = train.iloc[:, 2:]\n",
    "\n",
    "#load xgboost feature Pfams for training data that includes \n",
    "#contamination and low sequence transcriptomes\n",
    "features = pd.read_csv('Extracted_Pfams_contaminationLowSeqsIncluded_xgModel_xgFeatures.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract pfam column from features dataframe\n",
    "features = features['pfam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to encode trophic labels as numbers (0,1,2)\n",
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get just data for the feature Pfams from training data\n",
    "trainData = trainData[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign feature matrix and target vector\n",
    "X, y = trainData, le.fit_transform(train['Trophic mode'])\n",
    "#X: Feature matrix (independent variables) from the DataFrame trainData\n",
    "#y: Target vector (dependent variable), where the 'Trophic mode' column is label-encoded using `LabelEncoder`.\n",
    "\n",
    "#initialize a MinMaxScaler instance\n",
    "scaler = MinMaxScaler()\n",
    "#minMaxScaler scales features to a specified range, typically [0, 1], which can improve the performance of machine learning models\n",
    "\n",
    "#scale feature matrix\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the dataset into training and testing subsets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,           # Features (input variables)\n",
    "    y,           # Target variable (output/labels)\n",
    "    test_size=0.4, # Proportion of the dataset to include in the test split (40%)\n",
    "    random_state=0 # Random seed for reproducibility of the split\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(7)\n",
    "\n",
    "#{'gamma': 0.5, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'reg_lambda': 0.5}  \n",
    "clf = XGBClassifier(gamma=0.5, learning_rate=0.1, max_depth=3, n_estimators=100, reg_lambda=0.5)\n",
    "\n",
    "#define a custom scoring function for F1 score\n",
    "f1_scorer = make_scorer(f1_score, average='weighted')\n",
    "\n",
    "#create a StratifiedShuffleSplit cross-validator\n",
    "kf = StratifiedShuffleSplit(n_splits=6, random_state=7)\n",
    "\n",
    "#perform cross-validation and store the F1 scores\n",
    "f1_scores = cross_val_score(clf, X, y, cv=kf, scoring=f1_scorer)\n",
    "\n",
    "#save F1 scores\n",
    "data = {'F1_Scores': f1_scores}\n",
    "\n",
    "#calculate mean and standard error of F1 scores\n",
    "mean_f1 = data['F1_Scores'].mean()\n",
    "std_error_f1 = data['F1_Scores'].std(ddof=1) / np.sqrt(len(data['F1_Scores']))\n",
    "\n",
    "#create a new DataFrame with the results\n",
    "df = pd.DataFrame({\n",
    "    'Model': ['xgboost model, xgboost feature Pfams, training data with contamination and low sequence abundance, updated software'],\n",
    "    'Mean_F1_Score': [mean_f1],\n",
    "    'Std_Error_F1_Score': [std_error_f1]\n",
    "})\n",
    "\n",
    "#define output csv path\n",
    "csv_file_path = 'model_overall_f1_scores.csv'\n",
    "\n",
    "#check if the file already exists\n",
    "if os.path.exists(csv_file_path):\n",
    "    #append to the file without writing the header\n",
    "    df.to_csv(csv_file_path, mode='a', header=False, index=False)\n",
    "else:\n",
    "    #write the DataFrame as a new file with header\n",
    "    df.to_csv(csv_file_path, mode='w', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(7)\n",
    "\n",
    "#{'gamma': 0.5, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'reg_lambda': 0.5} \n",
    "clf = XGBClassifier(gamma=0.5, learning_rate=0.1, max_depth=3, n_estimators=100, reg_lambda=0.5)\n",
    "\n",
    "#define a custom scoring function for F1 score\n",
    "def f1_scorer(y_true, y_pred):\n",
    "    return f1_score(y_true, y_pred, average=None)\n",
    "\n",
    "#create a StratifiedShuffleSplit cross-validator\n",
    "kf = StratifiedShuffleSplit(n_splits=6, random_state=7)\n",
    "\n",
    "#initialize an empty list to store F1 scores for each class\n",
    "all_f1_scores = []\n",
    "\n",
    "#perform cross-validation and store the F1 scores for each fold\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    fold_f1_scores = f1_scorer(y_test, y_pred)\n",
    "    all_f1_scores.append(fold_f1_scores)\n",
    "\n",
    "#convert the list of arrays to a numpy array\n",
    "f1_scores = np.array(all_f1_scores)\n",
    "\n",
    "#save the F1 scores for each trophic mode separately\n",
    "trophic_names = [f'Trophic_{i}' for i in range(f1_scores.shape[1])]\n",
    "data = {trophic_name: f1_scores[:, i] for i, trophic_name in enumerate(trophic_names)}\n",
    "\n",
    "#prepare a list to store results\n",
    "results = []\n",
    "\n",
    "#loop through the data dictionary and calculate mean and standard error for each class\n",
    "for class_name, f1_scores in data.items():\n",
    "    mean_f1 = np.mean(f1_scores)\n",
    "    std_error_f1 = np.std(f1_scores, ddof=1) / np.sqrt(len(f1_scores))  # Standard error\n",
    "    results.append({\n",
    "        'Model': 'xgboost model, xgboost feature Pfams, training data with contamination and low sequence abundance, updated software',\n",
    "        'Trophic mode': class_name,\n",
    "        'Mean_F1_Score': mean_f1,\n",
    "        'Std_Error_F1_Score': std_error_f1\n",
    "    })\n",
    "\n",
    "#create a DataFrame from the results\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Replace Trophic values with desired labels\n",
    "df['Trophic mode'] = df['Trophic mode'].replace({'Trophic_0': 'Het', 'Trophic_1': 'Mix', 'Trophic_2': 'Phot'})\n",
    "\n",
    "#define output csv path\n",
    "csv_file_path = 'models_byClass_f1_scores.csv'\n",
    "\n",
    "#check if the file already exists\n",
    "if os.path.exists(csv_file_path):\n",
    "    #append to the file without writing the header\n",
    "    df.to_csv(csv_file_path, mode='a', header=False, index=False)\n",
    "else:\n",
    "    #write the DataFrame as a new file with header\n",
    "    df.to_csv(csv_file_path, mode='w', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random forest model, random forest features\n",
    "#training data with contamination or low sequence abundance \n",
    "#updated software"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load random forest feature Pfams for training data that includes \n",
    "#contamination and low sequence transcriptomes\n",
    "features = pd.read_csv('Extracted_Pfams_contaminationLowSeqsIncluded_rfModel_rfFeatures.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract pfam column from features dataframe\n",
    "features = features['pfam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get just the TPM values not the MMETSP entry IDs and trophic mode labels\n",
    "#same training data as above\n",
    "trainData = train.iloc[:, 2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to encode trophic labels as numbers (0,1,2)\n",
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get just data for the feature Pfams from training data\n",
    "trainData = trainData[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign feature matrix and target vector\n",
    "X, y = trainData, le.fit_transform(train['Trophic mode'])\n",
    "#X: Feature matrix (independent variables) from the DataFrame trainData\n",
    "#y: Target vector (dependent variable), where the 'Trophic mode' column is label-encoded using `LabelEncoder`.\n",
    "\n",
    "#initialize a MinMaxScaler instance\n",
    "scaler = MinMaxScaler()\n",
    "#minMaxScaler scales features to a specified range, typically [0, 1], which can improve the performance of machine learning models\n",
    "\n",
    "#scale feature matrix\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the dataset into training and testing subsets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,           # Features (input variables)\n",
    "    y,           # Target variable (output/labels)\n",
    "    test_size=0.4, # Proportion of the dataset to include in the test split (40%)\n",
    "    random_state=0 # Random seed for reproducibility of the split\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(7)\n",
    "\n",
    "#{'max_depth': None, 'min_samples_leaf': 3, 'min_samples_split': 5, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 1000} \n",
    "clf = RandomForestClassifier(max_depth=None,min_samples_leaf=3,min_samples_split=5,min_weight_fraction_leaf=0.0,n_estimators=1000)\n",
    "\n",
    "#define a custom scoring function for F1 score\n",
    "f1_scorer = make_scorer(f1_score, average='weighted')\n",
    "\n",
    "#create a StratifiedShuffleSplit cross-validator\n",
    "kf = StratifiedShuffleSplit(n_splits=6, random_state=7)\n",
    "\n",
    "#perform cross-validation and store the F1 scores\n",
    "f1_scores = cross_val_score(clf, X, y, cv=kf, scoring=f1_scorer)\n",
    "\n",
    "#save F1 scores\n",
    "data = {'F1_Scores': f1_scores}\n",
    "\n",
    "#calculate mean and standard error of F1 scores\n",
    "mean_f1 = data['F1_Scores'].mean()\n",
    "std_error_f1 = data['F1_Scores'].std(ddof=1) / np.sqrt(len(data['F1_Scores']))\n",
    "\n",
    "#create a new DataFrame with the results\n",
    "df = pd.DataFrame({\n",
    "    'Model': ['random forest model, random forest feature Pfams, training data with contamination and low sequence abundance, updated software'],\n",
    "    'Mean_F1_Score': [mean_f1],\n",
    "    'Std_Error_F1_Score': [std_error_f1]\n",
    "})\n",
    "\n",
    "#define output csv path\n",
    "csv_file_path = 'model_overall_f1_scores.csv'\n",
    "\n",
    "#check if the file already exists\n",
    "if os.path.exists(csv_file_path):\n",
    "    #append to the file without writing the header\n",
    "    df.to_csv(csv_file_path, mode='a', header=False, index=False)\n",
    "else:\n",
    "    #write the DataFrame as a new file with header\n",
    "    df.to_csv(csv_file_path, mode='w', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(7)\n",
    "\n",
    "#{'max_depth': None, 'min_samples_leaf': 3, 'min_samples_split': 5, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 1000} \n",
    "clf = RandomForestClassifier(max_depth=None,min_samples_leaf=3,min_samples_split=5,min_weight_fraction_leaf=0.0,n_estimators=1000)\n",
    "\n",
    "#define a custom scoring function for F1 score\n",
    "def f1_scorer(y_true, y_pred):\n",
    "    return f1_score(y_true, y_pred, average=None)\n",
    "\n",
    "#create a StratifiedShuffleSplit cross-validator\n",
    "kf = StratifiedShuffleSplit(n_splits=6, random_state=7)\n",
    "\n",
    "#initialize an empty list to store F1 scores for each class\n",
    "all_f1_scores = []\n",
    "\n",
    "#perform cross-validation and store the F1 scores for each fold\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    fold_f1_scores = f1_scorer(y_test, y_pred)\n",
    "    all_f1_scores.append(fold_f1_scores)\n",
    "\n",
    "#convert the list of arrays to a numpy array\n",
    "f1_scores = np.array(all_f1_scores)\n",
    "\n",
    "#save the F1 scores for each trophic mode separately\n",
    "trophic_names = [f'Trophic_{i}' for i in range(f1_scores.shape[1])]\n",
    "data = {trophic_name: f1_scores[:, i] for i, trophic_name in enumerate(trophic_names)}\n",
    "\n",
    "#prepare a list to store results\n",
    "results = []\n",
    "\n",
    "#loop through the data dictionary and calculate mean and standard error for each class\n",
    "for class_name, f1_scores in data.items():\n",
    "    mean_f1 = np.mean(f1_scores)\n",
    "    std_error_f1 = np.std(f1_scores, ddof=1) / np.sqrt(len(f1_scores))  # Standard error\n",
    "    results.append({\n",
    "        'Model': 'random forest model, random forest feature Pfams, training data with contamination and low sequence abundance, updated software',\n",
    "        'Trophic mode': class_name,\n",
    "        'Mean_F1_Score': mean_f1,\n",
    "        'Std_Error_F1_Score': std_error_f1\n",
    "    })\n",
    "\n",
    "#create a DataFrame from the results\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Replace Trophic values with desired labels\n",
    "df['Trophic mode'] = df['Trophic mode'].replace({'Trophic_0': 'Het', 'Trophic_1': 'Mix', 'Trophic_2': 'Phot'})\n",
    "\n",
    "#define output csv path\n",
    "csv_file_path = 'models_byClass_f1_scores.csv'\n",
    "\n",
    "#check if the file already exists\n",
    "if os.path.exists(csv_file_path):\n",
    "    #append to the file without writing the header\n",
    "    df.to_csv(csv_file_path, mode='a', header=False, index=False)\n",
    "else:\n",
    "    #write the DataFrame as a new file with header\n",
    "    df.to_csv(csv_file_path, mode='w', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xgboost model, xgboost features\n",
    "#training data without contamination or low sequence abundance\n",
    "#updated software\n",
    "#binary data rather than TPM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load training data and labels\n",
    "train = pd.read_csv('../trainingDataMarPRISM.csv')\n",
    "\n",
    "#get just the TPM values not the MMETSP entry IDs and trophic mode labels\n",
    "trainData = train.iloc[:, 2:]\n",
    "\n",
    "#load feature Pfams for model\n",
    "features = pd.read_csv('Extracted_Pfams_contaminationLowSeqsRemoved_xgModel_xgFeatures_binary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#covert training data TPM to binary\n",
    "trainData = (trainData > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract pfam column from features dataframe\n",
    "features = features['pfam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to encode trophic labels as numbers (0,1,2)\n",
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get just data for the feature Pfams from training data\n",
    "trainData = trainData[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign feature matrix and target vector\n",
    "X, y = trainData, le.fit_transform(train['Trophic mode'])\n",
    "#X: Feature matrix (independent variables) from the DataFrame trainData\n",
    "#y: Target vector (dependent variable), where the 'Trophic mode' column is label-encoded using `LabelEncoder`.\n",
    "\n",
    "#initialize a MinMaxScaler instance\n",
    "scaler = MinMaxScaler()\n",
    "#minMaxScaler scales features to a specified range, typically [0, 1], which can improve the performance of machine learning models\n",
    "\n",
    "#scale feature matrix\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the dataset into training and testing subsets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,           # Features (input variables)\n",
    "    y,           # Target variable (output/labels)\n",
    "    test_size=0.4, # Proportion of the dataset to include in the test split (40%)\n",
    "    random_state=0 # Random seed for reproducibility of the split\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(7)\n",
    "\n",
    "#{'gamma': 0.0, 'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 1000, 'reg_lambda': 0.5} \n",
    "clf = XGBClassifier(gamma=0.0, learning_rate=0.05, max_depth=3, n_estimators=1000, reg_lambda=0.5)\n",
    "\n",
    "#define a custom scoring function for F1 score\n",
    "f1_scorer = make_scorer(f1_score, average='weighted')\n",
    "\n",
    "#create a StratifiedShuffleSplit cross-validator\n",
    "kf = StratifiedShuffleSplit(n_splits=6, random_state=7)\n",
    "\n",
    "#perform cross-validation and store the F1 scores\n",
    "f1_scores = cross_val_score(clf, X, y, cv=kf, scoring=f1_scorer)\n",
    "\n",
    "#save F1 scores\n",
    "data = {'F1_Scores': f1_scores}\n",
    "\n",
    "#calculate mean and standard error of F1 scores\n",
    "mean_f1 = data['F1_Scores'].mean()\n",
    "std_error_f1 = data['F1_Scores'].std(ddof=1) / np.sqrt(len(data['F1_Scores']))\n",
    "\n",
    "#create a new DataFrame with the results\n",
    "df = pd.DataFrame({\n",
    "    'Model': ['xgboost model, xgboost feature Pfams, training data without contamination and low sequence abundance, updated software, binary'],\n",
    "    'Mean_F1_Score': [mean_f1],\n",
    "    'Std_Error_F1_Score': [std_error_f1]\n",
    "})\n",
    "\n",
    "#define output csv path\n",
    "csv_file_path = 'model_overall_f1_scores.csv'\n",
    "\n",
    "#check if the file already exists\n",
    "if os.path.exists(csv_file_path):\n",
    "    #append to the file without writing the header\n",
    "    df.to_csv(csv_file_path, mode='a', header=False, index=False)\n",
    "else:\n",
    "    #write the DataFrame as a new file with header\n",
    "    df.to_csv(csv_file_path, mode='w', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(7)\n",
    "\n",
    "#{'gamma': 0.0, 'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 1000, 'reg_lambda': 0.5} \n",
    "clf = XGBClassifier(gamma=0.0, learning_rate=0.05, max_depth=3, n_estimators=1000, reg_lambda=0.5)\n",
    "\n",
    "#define a custom scoring function for F1 score\n",
    "def f1_scorer(y_true, y_pred):\n",
    "    return f1_score(y_true, y_pred, average=None)\n",
    "\n",
    "#create a StratifiedShuffleSplit cross-validator\n",
    "kf = StratifiedShuffleSplit(n_splits=6, random_state=7)\n",
    "\n",
    "#initialize an empty list to store F1 scores for each class\n",
    "all_f1_scores = []\n",
    "\n",
    "#perform cross-validation and store the F1 scores for each fold\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    fold_f1_scores = f1_scorer(y_test, y_pred)\n",
    "    all_f1_scores.append(fold_f1_scores)\n",
    "\n",
    "#convert the list of arrays to a numpy array\n",
    "f1_scores = np.array(all_f1_scores)\n",
    "\n",
    "#save the F1 scores for each trophic mode separately\n",
    "trophic_names = [f'Trophic_{i}' for i in range(f1_scores.shape[1])]\n",
    "data = {trophic_name: f1_scores[:, i] for i, trophic_name in enumerate(trophic_names)}\n",
    "\n",
    "#prepare a list to store results\n",
    "results = []\n",
    "\n",
    "#loop through the data dictionary and calculate mean and standard error for each class\n",
    "for class_name, f1_scores in data.items():\n",
    "    mean_f1 = np.mean(f1_scores)\n",
    "    std_error_f1 = np.std(f1_scores, ddof=1) / np.sqrt(len(f1_scores))  # Standard error\n",
    "    results.append({\n",
    "        'Model': 'xgboost model, xgboost feature Pfams, training data without contamination and low sequence abundance, updated software, binary',\n",
    "        'Trophic mode': class_name,\n",
    "        'Mean_F1_Score': mean_f1,\n",
    "        'Std_Error_F1_Score': std_error_f1\n",
    "    })\n",
    "\n",
    "#create a DataFrame from the results\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "#replace Trophic values with desired labels\n",
    "df['Trophic mode'] = df['Trophic mode'].replace({'Trophic_0': 'Het', 'Trophic_1': 'Mix', 'Trophic_2': 'Phot'})\n",
    "\n",
    "#define output csv path\n",
    "csv_file_path = 'models_byClass_f1_scores.csv'\n",
    "\n",
    "#check if the file already exists\n",
    "if os.path.exists(csv_file_path):\n",
    "    #append to the file without writing the header\n",
    "    df.to_csv(csv_file_path, mode='a', header=False, index=False)\n",
    "else:\n",
    "    #write the DataFrame as a new file with header\n",
    "    df.to_csv(csv_file_path, mode='w', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xgboost model, xgboost features\n",
    "#training data without contamination or low sequence abundance\n",
    "#updated software\n",
    "#micromonas mix labels in training data coverted to phot labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load training data and labels\n",
    "train = pd.read_csv('trainingData_micromonasMixToPhot.csv')\n",
    "\n",
    "#get just the TPM values not the MMETSP entry IDs and trophic mode labels\n",
    "trainData = train.iloc[:, 2:]\n",
    "\n",
    "#load feature Pfams for model\n",
    "features = pd.read_csv('Extracted_Pfams_contaminationLowSeqsRemoved_xgModel_xgFeatures_micromonasMixToPhot.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract pfam column from features dataframe\n",
    "features = features['pfam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to encode trophic labels as numbers (0,1,2)\n",
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get just data for the feature Pfams from training data\n",
    "trainData = trainData[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign feature matrix and target vector\n",
    "X, y = trainData, le.fit_transform(train['Trophic mode'])\n",
    "#X: Feature matrix (independent variables) from the DataFrame trainData\n",
    "#y: Target vector (dependent variable), where the 'Trophic mode' column is label-encoded using `LabelEncoder`.\n",
    "\n",
    "#initialize a MinMaxScaler instance\n",
    "scaler = MinMaxScaler()\n",
    "#minMaxScaler scales features to a specified range, typically [0, 1], which can improve the performance of machine learning models\n",
    "\n",
    "#scale feature matrix\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the dataset into training and testing subsets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,           # Features (input variables)\n",
    "    y,           # Target variable (output/labels)\n",
    "    test_size=0.4, # Proportion of the dataset to include in the test split (40%)\n",
    "    random_state=0 # Random seed for reproducibility of the split\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(7)\n",
    "\n",
    "#{'gamma': 0.0, 'learning_rate': 0.1, 'max_depth': 10, 'n_estimators': 1000, 'reg_lambda': 1.0} \n",
    "clf = XGBClassifier(gamma=0.0, learning_rate=0.1, max_depth=10, n_estimators=1000, reg_lambda=1.0)\n",
    "\n",
    "#define a custom scoring function for F1 score\n",
    "f1_scorer = make_scorer(f1_score, average='weighted')\n",
    "\n",
    "#create a StratifiedShuffleSplit cross-validator\n",
    "kf = StratifiedShuffleSplit(n_splits=6, random_state=7)\n",
    "\n",
    "#perform cross-validation and store the F1 scores\n",
    "f1_scores = cross_val_score(clf, X, y, cv=kf, scoring=f1_scorer)\n",
    "\n",
    "#save F1 scores\n",
    "data = {'F1_Scores': f1_scores}\n",
    "\n",
    "#calculate mean and standard error of F1 scores\n",
    "mean_f1 = data['F1_Scores'].mean()\n",
    "std_error_f1 = data['F1_Scores'].std(ddof=1) / np.sqrt(len(data['F1_Scores']))\n",
    "\n",
    "#create a new DataFrame with the results\n",
    "df = pd.DataFrame({\n",
    "    'Model': ['xgboost model, xgboost feature Pfams, training data without contamination and low sequence abundance, updated software, micromonas mix to phot'],\n",
    "    'Mean_F1_Score': [mean_f1],\n",
    "    'Std_Error_F1_Score': [std_error_f1]\n",
    "})\n",
    "\n",
    "#define output csv path\n",
    "csv_file_path = 'model_overall_f1_scores.csv'\n",
    "\n",
    "#check if the file already exists\n",
    "if os.path.exists(csv_file_path):\n",
    "    #append to the file without writing the header\n",
    "    df.to_csv(csv_file_path, mode='a', header=False, index=False)\n",
    "else:\n",
    "    #write the DataFrame as a new file with header\n",
    "    df.to_csv(csv_file_path, mode='w', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(7)\n",
    "\n",
    "#{'gamma': 0.0, 'learning_rate': 0.1, 'max_depth': 10, 'n_estimators': 1000, 'reg_lambda': 1.0} \n",
    "clf = XGBClassifier(gamma=0.0, learning_rate=0.1, max_depth=10, n_estimators=1000, reg_lambda=1.0)\n",
    "\n",
    "#define a custom scoring function for F1 score\n",
    "def f1_scorer(y_true, y_pred):\n",
    "    return f1_score(y_true, y_pred, average=None)\n",
    "\n",
    "#create a StratifiedShuffleSplit cross-validator\n",
    "kf = StratifiedShuffleSplit(n_splits=6, random_state=7)\n",
    "\n",
    "#initialize an empty list to store F1 scores for each class\n",
    "all_f1_scores = []\n",
    "\n",
    "#perform cross-validation and store the F1 scores for each fold\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    fold_f1_scores = f1_scorer(y_test, y_pred)\n",
    "    all_f1_scores.append(fold_f1_scores)\n",
    "\n",
    "#convert the list of arrays to a numpy array\n",
    "f1_scores = np.array(all_f1_scores)\n",
    "\n",
    "#save the F1 scores for each trophic mode separately\n",
    "trophic_names = [f'Trophic_{i}' for i in range(f1_scores.shape[1])]\n",
    "data = {trophic_name: f1_scores[:, i] for i, trophic_name in enumerate(trophic_names)}\n",
    "\n",
    "#prepare a list to store results\n",
    "results = []\n",
    "\n",
    "#loop through the data dictionary and calculate mean and standard error for each class\n",
    "for class_name, f1_scores in data.items():\n",
    "    mean_f1 = np.mean(f1_scores)\n",
    "    std_error_f1 = np.std(f1_scores, ddof=1) / np.sqrt(len(f1_scores))  # Standard error\n",
    "    results.append({\n",
    "        'Model': ['xgboost model, xgboost feature Pfams, training data without contamination and low sequence abundance, updated software, micromonas mix to phot'],\n",
    "        'Trophic mode': class_name,\n",
    "        'Mean_F1_Score': mean_f1,\n",
    "        'Std_Error_F1_Score': std_error_f1\n",
    "    })\n",
    "\n",
    "#create a DataFrame from the results\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "#replace Trophic values with desired labels\n",
    "df['Trophic mode'] = df['Trophic mode'].replace({'Trophic_0': 'Het', 'Trophic_1': 'Mix', 'Trophic_2': 'Phot'})\n",
    "\n",
    "#define output csv path\n",
    "csv_file_path = 'models_byClass_f1_scores.csv'\n",
    "\n",
    "#check if the file already exists\n",
    "if os.path.exists(csv_file_path):\n",
    "    #append to the file without writing the header\n",
    "    df.to_csv(csv_file_path, mode='a', header=False, index=False)\n",
    "else:\n",
    "    #write the DataFrame as a new file with header\n",
    "    df.to_csv(csv_file_path, mode='w', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
